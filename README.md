<div align="center">

# ğŸ—ï¸ Offline Feature Backfill Platform

### Production-Grade Lakehouse Architecture for Feature Engineering

[![Python 3.8+](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Apache Spark](https://img.shields.io/badge/Spark-3.5.0-orange.svg)](https://spark.apache.org/)
[![Apache Iceberg](https://img.shields.io/badge/Iceberg-Lakehouse-green.svg)](https://iceberg.apache.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

*A unified offline feature platform built on **Apache Iceberg** + **Spark**, delivering **Time-Travel Reproducibility**, **WAP Governance**, and **Batch-Stream Consistency** for ML Feature Stores.*

[Features](#-core-features) â€¢ [Architecture](#-architecture) â€¢ [Quick Start](#-quick-start) â€¢ [Documentation](#-documentation)

</div>

---

## âœ¨ Why This Platform?

| Challenge | Our Solution |
|-----------|--------------|
| ğŸš« Dirty data polluting production | **WAP (Write-Audit-Publish)** isolates writes, validates before commit |
| ğŸ” "Which code produced this feature?" | **Code-Data Lineage** binds Git hash + Run ID to every snapshot |
| â° "What did the data look like last week?" | **Time Travel** queries any historical state instantly |
| ğŸ”„ Training/Serving skew | **Single Source of Truth** with incremental Online Store sync |

---

## ğŸš€ Core Features

### 1. ğŸ›¡ï¸ Data Governance & Quality (WAP Pattern)

**Zero dirty data in production.** The Write-Audit-Publish pattern ensures data quality at the gate:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“ WRITE        â”‚ â”€â”€â–¶ â”‚  ğŸ” AUDIT        â”‚ â”€â”€â–¶ â”‚  âœ… PUBLISH      â”‚
â”‚  Isolated Branch â”‚     â”‚  Auto Validation â”‚     â”‚  Fast-Forward    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **Write**: Backfill jobs write to an isolated `audit_<run_id>` branch
- **Audit**: Automated checks validate data distribution, null rates, and business rules
- **Publish**: Only validated data is atomically fast-forwarded to `main`

### 2. ğŸ•°ï¸ Full Reproducibility (Time Travel + Lineage)

Every row is traceable to the exact code, run, and time that produced it:

```sql
-- Query historical state
SELECT * FROM features FOR SYSTEM_VERSION AS OF '2023-01-01 10:00:00'

-- See full lineage
SELECT committed_at, 
       summary['spark.snapshot-property.code-version'] as git_hash,
       summary['spark.snapshot-property.run-id'] as run_id 
FROM features.snapshots
```

**Metadata Injected:**
- âœ… Git Commit Hash
- âœ… Unique Run ID  
- âœ… Pipeline User
- âœ… Timestamp

### 3. âš¡ Batch-Stream Consistency

No more training/serving skew:

- **Single Source of Truth**: Iceberg serves as the master store for historical correctness
- **Incremental Sync**: `OnlineSyncJob` reads validated deltas and upserts to Redis
- **Snapshot-based Delta**: Uses `start-snapshot-id` for efficient incremental reads

### 4. ğŸ”§ Self-Maintaining Lakehouse

Automated optimization keeps your data lake fast:

| Operation | Purpose | Default |
|-----------|---------|---------|
| `rewrite_data_files` | Compact small files | Auto |
| `expire_snapshots` | Reclaim storage | 7 days |
| `remove_orphan_files` | Clean unreferenced files | Auto |

---

## ï¿½ï¸ Architecture

```mermaid
graph TB
    subgraph "Data Sources"
        RAW[ğŸ“Š Raw Events]
    end
    
    subgraph "Lakehouse (Apache Iceberg)"
        RAW -->|Spark Batch| AUDIT[ğŸ”€ Audit Branch]
        AUDIT -->|Validator| DECISION{âœ“ Pass?}
        DECISION -->|Yes| MAIN[ğŸ“¦ Main Table]
        DECISION -->|No| DROP[âš ï¸ Alert & Drop]
        MAIN --> SNAPSHOTS[ğŸ“œ Snapshots & Lineage]
    end
    
    subgraph "Model Management"
        REGISTRY[ğŸ“¦ Model Registry]
        SNAPSHOTS -.->|Feature Binding| REGISTRY
    end
    
    subgraph "Batch Processing"
        REGISTRY -->|Load Model| BATCH[ğŸ¤– Batch Prediction]
        MAIN -->|Read Features| BATCH
        BATCH -->|Write| PREDICTIONS[(ğŸ“Š Predictions)]
    end
    
    subgraph "Serving Layer"
        MAIN -->|Incremental Read| SYNC[ğŸ”„ Online Sync Job]
        SYNC -->|Upsert| REDIS[(ğŸš€ Redis)]
        REDIS --> INFERENCE[âš¡ Real-time Inference]
    end
    
    subgraph "Observability"
        METRICS[ğŸ“Š Metrics Collector]
        ALERTS[ğŸ”” Alert Manager]
        MAIN -.-> METRICS
        BATCH -.-> METRICS
        SYNC -.-> METRICS
        METRICS --> ALERTS
    end
```

<div align="center">
<img src="docs/lakehouse_architecture.png" alt="ML Feature Platform Architecture" width="800"/>
<br/>
<em>Complete ML Feature Platform: Data Ingestion â†’ Lakehouse â†’ Model Registry â†’ Batch/Online Serving â†’ Observability</em>
</div>

---

## ğŸ“‚ Project Structure

```
offline_data_infra/
â”œâ”€â”€ ğŸ“œ README.md                     # You are here
â”œâ”€â”€ ğŸ“‹ requirements.txt              # Python dependencies
â”‚
â”œâ”€â”€ ğŸš€ run_backfill.sh               # Execute WAP pipeline
â”œâ”€â”€ ğŸ”„ run_sync.sh                   # Sync to online store
â”œâ”€â”€ ğŸ”§ run_maintenance.sh            # Optimize & cleanup
â”œâ”€â”€ ğŸ¤– run_batch_predict.sh          # Batch inference job
â”œâ”€â”€ ğŸ“¦ run_registry_demo.sh          # Model Registry demo
â”œâ”€â”€ ğŸ“Š run_monitoring_demo.sh        # Monitoring demo
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â”œâ”€â”€ backfill_job.py          # ğŸ­ Main WAP Engine
â”‚   â”‚   â”‚                            #    Compute â†’ Validate â†’ Commit
â”‚   â”‚   â”œâ”€â”€ online_sync_job.py       # ğŸ”„ Incremental Redis Sync
â”‚   â”‚   â”œâ”€â”€ maintenance_job.py       # ğŸ§¹ Compaction & Expiration
â”‚   â”‚   â””â”€â”€ batch_predict_job.py     # ğŸ¤– Spark Batch Inference
â”‚   â”‚
â”‚   â”œâ”€â”€ model/                       # ğŸ“¦ Model Management
â”‚   â”‚   â””â”€â”€ registry.py              # Model Registry with Lineage
â”‚   â”‚
â”‚   â”œâ”€â”€ monitoring/                  # ğŸ“Š Observability
â”‚   â”‚   â”œâ”€â”€ metrics.py               # Prometheus Metrics Collector
â”‚   â”‚   â””â”€â”€ alerts.py                # Alerting Engine & Rules
â”‚   â”‚
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â””â”€â”€ user_features.py         # ğŸ“Š Feature Definitions
â”‚   â”‚
â”‚   â”œâ”€â”€ validation/
â”‚   â”‚   â””â”€â”€ validator.py             # âœ… Data Quality Rules
â”‚   â”‚
â”‚   â”œâ”€â”€ serving/
â”‚   â”‚   â””â”€â”€ online_store.py          # ğŸš€ Redis Client Wrapper
â”‚   â”‚
â”‚   â””â”€â”€ common/
â”‚       â”œâ”€â”€ spark_utils.py           # ğŸ”§ Spark Session Factory
â”‚       â””â”€â”€ git_utils.py             # ğŸ”— Git Lineage Utils
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ architecture.md              # ğŸ“– Deep Dive Documentation
â”‚
â”œâ”€â”€ model_registry/                  # ğŸ“ Model Artifacts & Metadata
â”œâ”€â”€ tests/                           # ğŸ§ª Test Suite
â””â”€â”€ warehouse/                       # ğŸ“ Local Iceberg Warehouse
```

---

## ğŸš¦ Quick Start

### Prerequisites

| Requirement | Version | Notes |
|-------------|---------|-------|
| Java | 8, 11, or 17 | Required for Spark |
| Python | 3.8+ | Tested on 3.9, 3.10 |
| pip | Latest | For dependency management |

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/offline_data_infra.git
cd offline_data_infra

# Install dependencies
pip install -r requirements.txt
```

### Run the Pipeline

#### Step 1: Backfill (Compute Phase)
Runs the full **WAP** lifecycle: generates data â†’ writes to audit branch â†’ validates â†’ publishes if safe.

```bash
./run_backfill.sh --start_date 2023-01-01 --end_date 2023-01-02
```

**Expected Output:**
```
Starting Backfill. RunID: a1b2c3d4, Commit: abc1234
Computing features for user_features...
Creating Audit Branch: audit_a1b2c3d4
Validation PASSED. Fast-forwarding main to audit branch...
Successfully Published to Main!
```

#### Step 2: Online Sync (Serve Phase)
Incrementally pushes validated features to the online serving layer.

```bash
./run_sync.sh
```

#### Step 3: Maintenance (Optimize Phase)
Keeps the Lakehouse performant with automatic cleanup.

```bash
./run_maintenance.sh --days 7
```

#### Step 4: Batch Prediction (Inference Phase)
Run batch inference using models from the Model Registry.

```bash
# Mock mode for demo (no real model required)
./run_batch_predict.sh --mock

# With a registered model
./run_batch_predict.sh --model_name user_ctr_model
```

---

## ğŸ“¦ Model Registry

Centralized model versioning with **feature-data lineage binding**:

```python
from src.model.registry import ModelRegistry

registry = ModelRegistry()

# Register model with feature snapshot binding
version = registry.register_model(
    name="user_ctr_model",
    model_path="./model.pkl",
    metrics={"auc": 0.92, "f1": 0.85},
    feature_snapshot_id="snap_abc123",  # Iceberg snapshot used for training
    git_commit_hash="abc1234"
)

# Promote to production
registry.promote_to_production("user_ctr_model", version.version)

# Get lineage for reproducibility
lineage = registry.get_model_lineage("user_ctr_model", "1.0.0")
```

**Key Features:**
- âœ… Semantic versioning (1.0.0, 1.0.1, ...)
- âœ… Feature snapshot binding (Iceberg integration)
- âœ… Model lifecycle (staging â†’ production â†’ archived)
- âœ… Artifact storage and hash verification
- âœ… Full lineage tracking (code + data)

```bash
# Run demo
./run_registry_demo.sh
```

---

## ğŸ“Š Monitoring & Alerting

Production-grade observability with Prometheus-compatible metrics:

```python
from src.monitoring.metrics import MetricsCollector
from src.monitoring.alerts import AlertManager, create_default_alert_rules

# Collect metrics
collector = MetricsCollector("ml_platform")
collector.record_prediction_latency(0.05)
collector.record_feature_null_rate("user_id", 0.01)
collector.record_job_success("backfill")

# Export to Prometheus format
print(collector.export_prometheus())

# Set up alerting
manager = AlertManager()
for rule in create_default_alert_rules():
    manager.add_rule(rule)

# Evaluate and fire alerts
fired = manager.evaluate_all(collector)
```

**Pre-built Alert Rules:**
| Alert | Threshold | Severity |
|-------|-----------|----------|
| `high_feature_null_rate` | > 10% | âš ï¸ Warning |
| `prediction_latency_high` | P99 > 500ms | ğŸ”´ Error |
| `job_failure_spike` | > 3 failures | ğŸš¨ Critical |
| `feature_drift_detected` | Drift score > 0 | âš ï¸ Warning |
| `model_too_old` | > 1 week | â„¹ï¸ Info |

```bash
# Run demo
./run_monitoring_demo.sh
```

---

## ğŸ“Š Key Technologies

| Component | Technology | Purpose |
|-----------|------------|---------|
| Storage | Apache Iceberg | ACID transactions, time travel, branching |
| Compute | Apache Spark 3.5 | Distributed feature computation |
| Online Store | Redis | Low-latency feature serving |
| Lineage | Git Integration | Code-data binding |

---

## ğŸ“– Documentation

| Document | Description |
|----------|-------------|
| [Architecture Guide](docs/architecture.md) | Deep dive into system design |

### Topics Covered:
- ğŸŒ¿ Iceberg Branching Strategies
- ğŸ“¸ Snapshot Isolation Mechanics  
- ğŸ”„ Schema Evolution Rules
- ğŸ“ˆ Performance Tuning Guide

---

## ğŸ—ºï¸ Roadmap

**Completed âœ…:**
- [x] **Model Registry** - Centralized model versioning with feature lineage
- [x] **Batch Prediction** - Spark-based batch inference pipeline
- [x] **Monitoring & Alerting** - Prometheus metrics + rule-based alerts

**In Progress ğŸ”„:**
- [ ] **Grafana Dashboards** - Pre-built dashboard templates
- [ ] **Feature SDK** - Python DSL for feature definitions
- [ ] **Schema Registry Integration** - Automated schema evolution validation
- [ ] **Kubernetes Deployment** - Helm charts for production

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<div align="center">

**Built with â¤ï¸ for Feature Engineering at Scale**

*If you find this useful, please â­ the repository!*

</div>
